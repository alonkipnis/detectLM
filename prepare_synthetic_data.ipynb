{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9f302d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T18:26:06.732236Z",
     "start_time": "2023-06-02T18:26:06.641012Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from multitest import MultiTest\n",
    "import pandas as pd\n",
    "\n",
    "from src.DetectLM import DetectLM\n",
    "from src.fit_survival_function import fit_per_length_survival_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading HC null values from HC_null_sim_results.csv...\n"
     ]
    }
   ],
   "source": [
    "from src.HC_survival_function import get_HC_survival_function\n",
    "HC_pval_func = get_HC_survival_function(HC_null_sim_file=\"HC_null_sim_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'get_HC_survival_function.<locals>.func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mexample/HC_pval_function.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(HC_pval_func, f)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'get_HC_survival_function.<locals>.func'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences':                                             sentence  response    pvalue  \\\n",
      "0                                    Example text.\\n  6.100810       NaN   \n",
      "1  The text below explains the default configurat...  4.303938       NaN   \n",
      "2  Some of the configurations can be easily adjus...  3.539569  0.571496   \n",
      "3  The most important parameter is the path to th...  4.495553  0.064241   \n",
      "4              The default sentence parser is spacy.  7.019018       NaN   \n",
      "5  This parser somtimes has unexpected behavior, ...  5.509454  0.010378   \n",
      "6  There is also an instruction to ignore sentenc...  4.451342  0.116835   \n",
      "7  Additionally, sentences that are too long are ...  3.315172  0.794605   \n",
      "\n",
      "  context                         comment   mask  \n",
      "0    None  ignored (below minimal length)  False  \n",
      "1    None  ignored (below minimal length)  False  \n",
      "2    None                              OK  False  \n",
      "3    None                              OK   True  \n",
      "4    None  ignored (below minimal length)  False  \n",
      "5    None                              OK   True  \n",
      "6    None                              OK  False  \n",
      "7    None                              OK  False  , 'HC': 1.3396600337668725, 'fisher': 20.49921190930749, 'fisher_pvalue': 0.02486927492187124}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from text_detect import get_survival_function\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM    \n",
    "from src.PerplexityEvaluator import PerplexityEvaluator\n",
    "from src.DetectLM import DetectLM\n",
    "from src.PrepareSentenceContext import PrepareSentenceContext\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "INPUT_FILE = 'example_text.txt'\n",
    "\n",
    "# Load the logloss p-value function. Ususally one must fit this function using triaining data\n",
    "# from the null class and ``fit_survival_function``.\n",
    "# Here we load a pre-fitted function for the GPT-2 language model under Wikipedia-Introduction \n",
    "# dataset and no context.\n",
    "with open('example/logloss_pval_function.pkl', 'rb') as f:\n",
    "    pval_function = pickle.load(f)\n",
    "\n",
    "# Initialize PerplexityEvaluator with a language model and a tokenizer\n",
    "lm_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_name)\n",
    "\n",
    "sentence_detector = PerplexityEvaluator(AutoModelForCausalLM.from_pretrained(lm_name),\n",
    "                    AutoTokenizer.from_pretrained(lm_name))\n",
    "\n",
    "# initialize the detector...\n",
    "detector = DetectLM(sentence_detector, pval_function,\n",
    "                    min_len=8, max_len=50, length_limit_policy='truncate')\n",
    "\n",
    "# parse text from an input file \n",
    "with open(INPUT_FILE, 'rt') as f:\n",
    "    text = f.read()\n",
    "parse_chunks = PrepareSentenceContext(context_policy=None)\n",
    "chunks = parse_chunks(text)\n",
    "\n",
    "# Test document\n",
    "res = detector(chunks['text'], chunks['context'])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectBivariateSpline\n",
    "import numpy as np\n",
    "ll = np.arange(5, 500)\n",
    "xx0 = np.arange(0.1, 10, 0.01)\n",
    "\n",
    "ll_valid = []\n",
    "zz = []\n",
    "for l in ll:\n",
    "    ll_valid.append(l)\n",
    "    zz.append(HC_pval_func(l, xx0)[0])\n",
    "\n",
    "func = RectBivariateSpline(ll_valid, xx0, np.vstack(zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77396079]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(100, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1065159]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_valid = res['sentences']['pvalue'].dropna().shape[0]\n",
    "func(num_valid ,res['HC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salaries:  0.48\n",
      "Equipment:  0.06666666666666667\n",
      "Supplies:  0.06666666666666667\n",
      "Travel:  0.2\n",
      "Other Expenses:  0.09333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Salaries: \", 18000/37500)\n",
    "print(\"Equipment: \", 2500/37500)\n",
    "print(\"Supplies: \", 2500/37500)\n",
    "print(\"Travel: \", 7500/37500)\n",
    "print(\"Other Expenses: \", 3500/37500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"example/HC_pval_function.pkl\", \"wb\") as f:\n",
    "    pickle.dump(func, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'fit_per_length_survival_function.<locals>.func2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpkl\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mexample/example_pval_function.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pkl\u001b[39m.\u001b[39;49mdump(pval_function, f)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'fit_per_length_survival_function.<locals>.func2d'"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open('example/example_pval_function.pkl', 'wb') as f:\n",
    "    pkl.dump(pval_function, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/synthetic_data_HV_vals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kipnisal/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/kipnisal/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/numpy/core/_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for c in df.groupby(['dataset_name', 'min_length', 'itr']):\n",
    "    #print(c[0])\n",
    "    HC_null = c[1].loc[c[1]['epsilon'] == 0.0, 'HC'].values\n",
    "    eps_vals = df['epsilon'].unique()\n",
    "    for eps_val in eps_vals:\n",
    "        if eps_val > 0:\n",
    "            HC_alt = c[1].loc[c[1]['epsilon'] == eps_val, 'HC'].values\n",
    "            tt = np.linspace(HC_null.min(), HC_null.max(), 1000)\n",
    "            P1 = np.mean(np.expand_dims(HC_alt, 1) > np.expand_dims(tt, 0), 0)\n",
    "            P2 = np.mean(np.expand_dims(HC_null, 1) <= np.expand_dims(tt, 0), 0)\n",
    "            #print(f\"eps = {eps_val}, acc_t = {np.max(P1 + P2)/ 2}\")\n",
    "            res.append({'dataset_name': c[0][0] ,'min_length': c[0][1],'itr': c[0][2], 'epsilon': eps_val, 'acc_t': np.max(P1 + P2)/ 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>min_length</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">gpt2-xl</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">50</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">50</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.2</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">50</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      mean   std\n",
       "model   epsilon min_length dataset              \n",
       "gpt2-xl 0.0     50         abstracts  0.53  0.01\n",
       "                           news       0.55  0.00\n",
       "                           wiki-long  0.53  0.01\n",
       "                100        abstracts  0.54  0.01\n",
       "                           news       0.56  0.01\n",
       "                           wiki-long  0.54  0.01\n",
       "                200        abstracts  0.54  0.01\n",
       "                           news       0.56  0.01\n",
       "                           wiki-long  0.54  0.01\n",
       "        0.1     50         abstracts  0.64  0.01\n",
       "                           news       0.61  0.01\n",
       "                           wiki-long  0.64  0.01\n",
       "                100        abstracts  0.69  0.02\n",
       "                           news       0.65  0.01\n",
       "                           wiki-long  0.69  0.02\n",
       "                200        abstracts  0.76  0.02\n",
       "                           news       0.71  0.01\n",
       "                           wiki-long  0.76  0.01\n",
       "        0.2     50         abstracts  0.76  0.01\n",
       "                           news       0.69  0.01\n",
       "                           wiki-long  0.77  0.01\n",
       "                100        abstracts  0.84  0.01\n",
       "                           news       0.76  0.01\n",
       "                           wiki-long  0.84  0.01\n",
       "                200        abstracts  0.92  0.01\n",
       "                           news       0.83  0.01\n",
       "                           wiki-long  0.91  0.01"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/kipnisal/Dropbox/Research/AuthorshipAI/results/synthetic_data_report_al_02_nMonte_30.csv')\n",
    "df.groupby(['model', 'epsilon', 'min_length', 'dataset']).accuracy.agg(['mean', 'std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "797    29\n",
       "798    29\n",
       "799    29\n",
       "800    29\n",
       "801    29\n",
       "Name: itr, Length: 802, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epsilon</th>\n",
       "      <th>min_length</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">50</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.498384</td>\n",
       "      <td>0.008711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.497092</td>\n",
       "      <td>0.002872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.500461</td>\n",
       "      <td>0.003835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.495488</td>\n",
       "      <td>0.007389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.501517</td>\n",
       "      <td>0.005121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.499231</td>\n",
       "      <td>0.005802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.506128</td>\n",
       "      <td>0.007490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.501390</td>\n",
       "      <td>0.006689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.500308</td>\n",
       "      <td>0.009654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">50</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.604824</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.605097</td>\n",
       "      <td>0.003604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.665572</td>\n",
       "      <td>0.009670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.651330</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.653058</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.728858</td>\n",
       "      <td>0.009104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.715613</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.720330</td>\n",
       "      <td>0.006553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.795399</td>\n",
       "      <td>0.008432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0.2</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">50</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.663256</td>\n",
       "      <td>0.006339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.673669</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.755686</td>\n",
       "      <td>0.007854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.723477</td>\n",
       "      <td>0.007312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.737004</td>\n",
       "      <td>0.004618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.005793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>abstracts</th>\n",
       "      <td>0.783648</td>\n",
       "      <td>0.006381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.005284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki-long</th>\n",
       "      <td>0.863814</td>\n",
       "      <td>0.005231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  mean       std\n",
       "epsilon min_length dataset                      \n",
       "0.0     50         abstracts  0.498384  0.008711\n",
       "                   news       0.497092  0.002872\n",
       "                   wiki-long  0.500461  0.003835\n",
       "        100        abstracts  0.495488  0.007389\n",
       "                   news       0.501517  0.005121\n",
       "                   wiki-long  0.499231  0.005802\n",
       "        200        abstracts  0.506128  0.007490\n",
       "                   news       0.501390  0.006689\n",
       "                   wiki-long  0.500308  0.009654\n",
       "0.1     50         abstracts  0.604824  0.009592\n",
       "                   news       0.605097  0.003604\n",
       "                   wiki-long  0.665572  0.009670\n",
       "        100        abstracts  0.651330  0.006500\n",
       "                   news       0.653058  0.003968\n",
       "                   wiki-long  0.728858  0.009104\n",
       "        200        abstracts  0.715613  0.012200\n",
       "                   news       0.720330  0.006553\n",
       "                   wiki-long  0.795399  0.008432\n",
       "0.2     50         abstracts  0.663256  0.006339\n",
       "                   news       0.673669  0.001574\n",
       "                   wiki-long  0.755686  0.007854\n",
       "        100        abstracts  0.723477  0.007312\n",
       "                   news       0.737004  0.004618\n",
       "                   wiki-long  0.821285  0.005793\n",
       "        200        abstracts  0.783648  0.006381\n",
       "                   news       0.810574  0.005284\n",
       "                   wiki-long  0.863814  0.005231"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'results/synthetic_data_report_al_20.csv'\n",
    "#file_name = 'results/synthetic_data_report_al_0005.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "level = df[df['epsilon'] == 0.0].groupby('dataset')['detection_rate'].mean()\n",
    "df =df.join(level, rsuffix='_null_mean', on='dataset')\n",
    "df['accuracy'] = (1 - df['detection_rate_null_mean'] + df['detection_rate'])/2\n",
    "\n",
    "df.groupby(['epsilon','min_length', 'dataset']).accuracy.agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Testing Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992d1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wiki-bio'\n",
    "model_name = \"gpt2-xl\"\n",
    "\n",
    "params = {}\n",
    "params['ignore-first-sentence'] = True\n",
    "params['null-data-file'] = f\"results/{model_name}_no_context_{dataset_name}_machine.csv\"\n",
    "params['language-model-name'] = model_name\n",
    "params['number-of-interpolation-points'] = 47\n",
    "params['max-tokens-per-sentence'] = 50\n",
    "params['min-tokens-per-sentence'] = 8\n",
    "params['hc-type'] = \"stbl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe308a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/gpt2-xl_no_context_wiki-bio_machine.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     xx1 \u001b[39m=\u001b[39m df1[value_name]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_per_length_survival_function(ll, xx1, log_space\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, G\u001b[39m=\u001b[39mG)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df_null \u001b[39m=\u001b[39m get_null_data(params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pval_functions \u001b[39m=\u001b[39m get_survival_function(df_null, G\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mnumber-of-interpolation-points\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_null_data\u001b[39m(params):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df_null \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(params[\u001b[39m'\u001b[39;49m\u001b[39mnull-data-file\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m params[\u001b[39m'\u001b[39m\u001b[39mignore-first-sentence\u001b[39m\u001b[39m'\u001b[39m]: \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kipnisal/Dropbox/Research/AuthorshipAI/prepare_synthetic_data.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         df_null \u001b[39m=\u001b[39m df_null[df_null\u001b[39m.\u001b[39mnum \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Dropbox/Research/AuthorshipAI/env/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/gpt2-xl_no_context_wiki-bio_machine.csv'"
     ]
    }
   ],
   "source": [
    "def get_null_data(params):\n",
    "    df_null = pd.read_csv(params['null-data-file'])\n",
    "    if params['ignore-first-sentence']: \n",
    "        df_null = df_null[df_null.num > 1]\n",
    "    return df_null\n",
    "\n",
    "def get_survival_function(df, G=101):\n",
    "    \"\"\"\n",
    "    One survival function for every sentence length in tokens\n",
    "\n",
    "    Args:\n",
    "    :df:  data frame with columns 'response' and 'length'\n",
    "\n",
    "    Return:\n",
    "        bivariate function (length, responce) -> (0,1)\n",
    "\n",
    "    \"\"\"\n",
    "    assert not df.empty\n",
    "    value_name = \"response\" if \"response\" in df.columns else \"logloss\"\n",
    "\n",
    "    df1 = df[~df[value_name].isna()]\n",
    "    ll = df1['length']\n",
    "    xx1 = df1[value_name]\n",
    "    return fit_per_length_survival_function(ll, xx1, log_space=True, G=G)\n",
    "\n",
    "       \n",
    "df_null = get_null_data(params)\n",
    "pval_functions = get_survival_function(df_null, G=params['number-of-interpolation-points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a3d3cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_machine = pd.read_csv(f\"results/{model_name}_no_context_{dataset_name}_machine.csv\")\n",
    "ds_human = pd.read_csv(f\"results/{model_name}_no_context_{dataset_name}_human.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "80e9af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117746\n",
      "361391\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "5c6c25ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9780\n",
      "0.07752279215394678\n"
     ]
    }
   ],
   "source": [
    "eps = 0.1\n",
    "\n",
    "ds_merged = ds_machine.merge(ds_human, on='name', how='inner')\n",
    "\n",
    "joint_names = ds_merged['name'].tolist()\n",
    "\n",
    "\n",
    "ds_pool = ds_human[ds_human['name'].isin(joint_names)]\n",
    "ds_sample = ds_pool.groupby(\"name\").sample(frac=eps)\n",
    "\n",
    "print(len(ds_sample))\n",
    "\n",
    "ds_sample['human'] = True\n",
    "ds_machine['human'] = False\n",
    "ds_mixed = pd.concat([ds_machine[ds_machine['name'].isin(joint_names)], ds_sample])\n",
    "\n",
    "print(ds_mixed.groupby('name')['human'].mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "871816ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_articles_to_minimum_length(df, min_length):\n",
    "    \"\"\"\n",
    "    Rearrange group names so that every group has at least\n",
    "    :min_length: elements\n",
    "    \"\"\"\n",
    "    df_grouped = df.copy()\n",
    "    df_grouped.loc[:, 'new_name'] = df['name'].copy()\n",
    "    groups = list(ds_mixed.groupby('name'))\n",
    "    lo_names = []\n",
    "    while len(groups) > 0:\n",
    "        c = groups.pop(0)\n",
    "        acc = len(c[1])\n",
    "        while (acc <= min_length) and len(groups)>0:\n",
    "            c1 = groups.pop(0)\n",
    "            acc += len(c1[1])\n",
    "            df_grouped.loc[df['name'] == c1[0], 'new_name'] = c[0]\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "min_length = 100\n",
    "ds_mixed_grouped = group_articles_to_minimum_length(ds_mixed, min_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ac81684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3304/3304 [00:01<00:00, 2527.09it/s]\n"
     ]
    }
   ],
   "source": [
    "detectlm = DetectLM(lambda x: 0,\n",
    "                    pval_functions,\n",
    "                     min_len=params['min-tokens-per-sentence'],\n",
    "                    max_len=params['max-tokens-per-sentence'],\n",
    "                    HC_type=params['hc-type'],\n",
    "                    ignore_first_sentence=params['ignore-first-sentence']\n",
    "                      )\n",
    "stbl = True if params['hc-type']=='stbl' else False\n",
    "\n",
    "min_no_sentences = 10\n",
    "\n",
    "results = []\n",
    "too_short = []\n",
    "for c in tqdm(ds_mixed_grouped.groupby('new_name')):\n",
    "    responses = c[1]['response']\n",
    "    lengths = c[1]['length']\n",
    "    if len(responses) > min_no_sentences:\n",
    "      pvals, comments = detectlm._get_pvals(responses, lengths)\n",
    "      pvals = np.vstack(pvals).squeeze()\n",
    "      mt = MultiTest(pvals, stbl=stbl)\n",
    "      hc = mt.hc()[0]\n",
    "      results.append(dict(id=c[0], HC=hc))\n",
    "    else:\n",
    "       too_short.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a3d34eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_vals = pd.read_csv(\"HC_critvals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b7c43be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=gpt2-xl, dataset=news, epsilon=0.1, length=100, sig_level=0.05 --> detection rate 0.3662227602905569\n"
     ]
    }
   ],
   "source": [
    "sig_level = 0.05\n",
    "t0 = crit_vals[(crit_vals.n == min_length) & (crit_vals.alpha == sig_level)].q_alpha.values[0]\n",
    "acc = np.mean(pd.DataFrame.from_dict(results)['HC'] > t0)\n",
    "\n",
    "print(f\"Model={model_name}, dataset={dataset_name}, epsilon={eps}, length={min_length}, sig_level={sig_level} --> detection rate {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c597c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3aabef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f317b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4b354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
