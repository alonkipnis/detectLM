# DetectLM: detect mixed authorship of a language model (LM) and humans

## Pipeline

### Initialization
In initialization, we tune the detector to a specific language model of known weights, specific texts generated by AI 
(preferably the actual model) considered as null data, and a specific context policy (what is used as a context for generation). 
The main ingredient we tune is the function with which we get P-values, i.e. the survival function of log-perplexity 
of AI-generated texts we assume to have. For better results, also get tune the survival function of Higher Criticism and
Fisher's combination statistic associated with full documents.        

#### The ingredients:
1. Causal language model. A mapping from context text to next tokens probabilities.
2. "Null data". Texts generated by the model or a related langauge model.
 
#### The steps:
1. Evaluate log-perplexity of individual sentences (or larger chunks) given a context policy. Here we want to record 
the length of every sentence. Use ``many_atomic_detections.py`` for this step. 
2. Fit survival function to the resulting log-perplexity, while taking into account the length (different function to 
each length). Check ``fit_pvalue_function.py`` to understand this step.   

### Inference
Here we apply the detector to a new document. The detector assigns a P-value to every sentence, returning the
statistics: Higher Criticism (HC) and Fisher's combination test. Under the null, the distribution of HC can be simulated
from the null data or via sampling P-values from the uniform distribution. For Fisher's combination statistic, you can 
use the chi-squared distribution or characterize the null by simulating from the null data. 

#### Ingredients:
1. Causal language model. A mapping from context text to next tokens probabilities.
2. Survival function of log-perplexity to evaluate per-sentence P-values. This function must match the context policy.

#### The steps:
1. Initialize a detector (``DetectLM``) with a log-perplexity function (``PerplexityEvaluator``) and a corresponding 
P-value function. 
2. Given a document, convert it to a list of (sentence, context) pairs (``PrepareSentenceContext``) 
3. Pass these pairs to the detector, which will return:
    (a) a list of sentences and their P-values.
    (b) HC test statistic
    (c) Fisher's test statistic
    (d) Chisquared test P-value for Fisher's statistic 
4. If HC or Fisher's test are too large, report "some sentences are not model-generated" and identify those sentences by
the ``mask`` column. Additional calibration may be needed to determine what is "too large".

## Modules
- ``PerplexityEvaluator``. Evaluate the log-perplexity (aka log-loss) of a (text, context) pair with respect to a given 
language model. For initialization, you'll need to provide a ``Huggingface`` tokenizer and model like 
``AutoTokenizer`` and ``AutoModelForCausalLM``.
- ``PrepareSentenceContext``. This module breaks a document into sentences and assigns a context to every sentence. For 
example, the context can be ``None`` or the previous sentence or the title. 
- ``DetectLM``. Given a list of sentences and possible contexts, returns HC test and Fisher test statistics indicating
whether the document contains parts written not by the model (large values of HC or Fisher indicate the involvement of a 
human). The result is obtained by applying ``PerplexityEvaluator`` to every (sentence, context) pair.
To initialize ``DetectLM``, you will need to provide a function to evaluate P-values. Typically, this function is 
evaluated by fitting a curve to the empirical survival function of perplexity of sentences under the model with a given 
context policy. It is also a good idea to take into account the length of the sentence because longer sentences tend to 
have smaller perplexity.    

## Scripts
- ``test_sentence_detector.py``. Computes the log perplexity of an input text.
- ``many_atomic_detections.py``. Evaluate the log perplexity of many sentences give a specific policy. This script can 
be useful to characterize P-value function or to analyze the power of the pipeline against a mixture from a specific 
domain.
- ``test_text_detect.py``. Apply the full testing pipeline to an input text file. This scripts loads "null data" and fit 
a function to evaluate P-values. To obtain reliable detection, the null data must be obtained under the same context
policy of the test. 
- ``fit_pvalue_function.py``. Report on the histogram of simulated null log-perplexities and the dependency of the 
- log-perplexity in the sentence's length. 

## Example
``

    from test_text_detect import get_pval_func_dict
    from transformers import AutoTokenizer, AutoModelForCausalLM    

    # Reading null data and fit p-value function for every sentence length
    pval_functions = get_pval_func_dict(...)

    # Initialize PerplexityEvaluator with a language model and a tokenizer
    lm_name = "gpt2"
    tokenizer = AutoTokenizer.from_pretrained(lm_name)
    
    sentence_detector = PerplexityEvaluator(AutoModelForCausalLM.from_pretrained(lm_name),
                        AutoTokenizer.from_pretrained(lm_name))
    
    # initialize the detector...
    max_len = np.max(list(pval_functions.keys()))
    detector = DetectLM(sentence_detector, pval_functions, context_policy=None,
                        max_len=max_len, length_limit_policy='truncate')
    
    # parse text from an input file 
    with open(INPUT_FILE, 'rt') as f:
        text = f.read()
    parse_chunks = PrepareSentenceContext(context_policy='previous_sentence')
    chunks = parse_chunks(text)
    
    # Test document
    res = detector(chunks['text'], chunks['context'])

``

